Pong RL ‚Äì Deep Q-Network Agent
================================

üìç Estado: En desarrollo üöß ¬∑ Funcionalidad central ‚úÖ ¬∑ Mejoras pendientes üîÑ

Descripci√≥n
-----------
Este proyecto implementa un agente de **Deep Q-Network (DQN)** que aprende a jugar al cl√°sico juego **Pong** mediante **Reinforcement Learning** usando **PyTorch**, **Gymnasium** y preprocesamiento de im√°genes.

Estructura de archivos (carpeta src/)
-------------------------------------
- `agent.py`             : Clase DQNAgent con selecci√≥n de acciones Œµ-greedy, entrenamiento y actualizaci√≥n de la red objetivo
- `model.py`             : Arquitectura de la red neuronal (conv layers + fully connected)
- `memory_buffer.py`     : Implementaci√≥n de ReplayBuffer para almacenar y muestrear experiencias
- `hyperparameters.py`   : Definici√≥n de hiperpar√°metros y envoltorio del entorno (AtariWrapper)
- `wrappers.py`          : C√≥digo de los wrappers Atari (frame skip, warp frame, clip reward, sticky actions, etc.)
- `train.py`             : Bucle de entrenamiento principal con logging en Weights & Biases (wandb)
- `main.py`              : Ejemplo para probar el entorno Pong y renderizar acciones aleatorias
 entrenado
- `requirements.txt`     : Dependencias del proyecto (completar)

Dependencias
------------
- Python 3.8+
- gymnasium
- torch
- numpy
- matplotlib
- supersuit
- stable-baselines3
- wandb

Instalaci√≥n r√°pida
------------------
1. Clona el repositorio:
   ```
   git clone <URL-del-repo>
   cd <repo>/src
   ```
2. Crea y activa un entorno virtual:
   ```
   python3 -m venv .venv
   source .venv/bin/activate
   ```
3. Instala las dependencias:
   ```
   pip install -r requirements.txt
   ```

Uso
---
1. **Entrenamiento**  
   Ejecuta el bucle de entrenamiento:
   ```
   python train.py
   ```
   - Registra m√©tricas en Weights & Biases.
   - Guarda pesos peri√≥dicamente en la carpeta `weights/`.

2. **Prueba del entorno**  
   Usa `main.py` para comprobar que el entorno funciona y renderiza:
   ```
   python main.py
   ```

3. **Evaluaci√≥n del agente**  
   Ejecuta (o crea) `test.py` para cargar pesos guardados y ver al agente jugar:
   ```
   python test.py
   ```

Configuraci√≥n de WandB
----------------------
Antes de entrenar, define tu API key:
```
export WANDB_API_KEY="TU_API_KEY"
```

Roadmap / Pr√≥ximos pasos
------------------------
- Visualizaci√≥n en tiempo real de m√©tricas de entrenamiento
- Afinar hiperpar√°metros (learning_rate, discount, batch_size‚Ä¶)
- Soporte para otros algoritmos (e.g. Double DQN, Dueling DQN, PPO)
- Refactorizar en m√≥dulos: separar Agent, Model, Environment y Entrenamiento
- Documentaci√≥n detallada y ejemplos de configuraci√≥n avanzada

Licencia
--------
MIT License ‚Äì consulta el archivo `LICENSE` para m√°s detalles.

Autor
-----
Xavier Mic√≥ ‚Äì 2025
